
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Case Study: HPC Mirroring &#8212; sarracenia 3.00.013 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/classic.css" />
    
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="nav-item nav-item-0"><a href="../index.html">sarracenia 3.00.013 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Case Study: HPC Mirroring</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="case-study-hpc-mirroring">
<h1><a class="toc-backref" href="#id2">Case Study: HPC Mirroring</a><a class="headerlink" href="#case-study-hpc-mirroring" title="Permalink to this headline">¶</a></h1>
<section id="continuously-mirror-27-million-file-tree-very-quickly">
<h2><a class="toc-backref" href="#id3">Continuously Mirror 27 Million File Tree Very Quickly</a><a class="headerlink" href="#continuously-mirror-27-million-file-tree-very-quickly" title="Permalink to this headline">¶</a></h2>
<div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#case-study-hpc-mirroring" id="id2">Case Study: HPC Mirroring</a></p>
<ul>
<li><p><a class="reference internal" href="#continuously-mirror-27-million-file-tree-very-quickly" id="id3">Continuously Mirror 27 Million File Tree Very Quickly</a></p>
<ul>
<li><p><a class="reference internal" href="#summary" id="id4">Summary</a></p></li>
<li><p><a class="reference internal" href="#problem-statement" id="id5">Problem Statement</a></p></li>
<li><p><a class="reference internal" href="#hpcr-solution-overview" id="id6">HPCR Solution Overview</a></p></li>
<li><p><a class="reference internal" href="#continuous-mirroring" id="id7">Continuous Mirroring</a></p></li>
<li><p><a class="reference internal" href="#reading-the-tree-takes-too-long" id="id8">Reading the Tree Takes Too Long</a></p></li>
<li><p><a class="reference internal" href="#detection-methods-inotify-policy-shim" id="id9">Detection Methods: Inotify, Policy, SHIM</a></p>
<ul>
<li><p><a class="reference internal" href="#shim-library" id="id10">Shim Library</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#copying-files" id="id11">Copying Files</a></p></li>
<li><p><a class="reference internal" href="#shim-library-necessary" id="id12">Shim Library Necessary</a></p></li>
<li><p><a class="reference internal" href="#does-it-work" id="id13">Does it Work?</a></p></li>
<li><p><a class="reference internal" href="#is-it-fast" id="id14">Is it Fast?</a></p></li>
<li><p><a class="reference internal" href="#overheads" id="id15">Overheads</a></p></li>
<li><p><a class="reference internal" href="#contributions" id="id16">Contributions</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<section id="summary">
<h3><a class="toc-backref" href="#id4">Summary</a><a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h3>
<p>This project has taken longer than expected, over three years, as the problem space was explored with the
help of a very patient client while the tool to design and implement the efficient solution was eventually
settled on. The client asked for a solution to make files available on the backup cluster within five
minutes of their creation on the primary one, and the first version of mirroring, deployed in 2017,
achieves roughly a 20 minute delay.  Version 2 was deployed in January 2020.</p>
<p>The client is actually more of a partner, who had very large test cases available and
ended up shouldering the responsibility for all of us to understand whether the solution was working or not.
While there are many specificities of this implementation, the resulting tool relies on no specific features
beyond a normal Linux file system to achieve a 72:1 speedup compared to rsync on real-time continuous
mirroring of 16 terabytes in 1.9 million files per day between two trees of 27 million files each. While
this averages to 185 Mbytes/second sustained over a 24 hour period. It should be noted that the transfers
are very peaky. On the same equipment at the same time, another
4 terabytes per day is being written to clusters on another network, so the aggregate read rate on
the operational cluster is 20 Terabytes per day (231 mbytes/second) for this application, while
the file systems are used for all the normal user applications as well.</p>
<p>While this project had to suffer through the development, with the lessons learned and the tools
now available, it should be straightforward to apply this solution to other cases. The end result is
that one adds a <a class="reference external" href="https://en.wikipedia.org/wiki/Shim_(computing)">Shim Library</a> to the users’ environment (transparent to user jobs), and
then every time a file is written, an AMQP message with file metadata is posted. A pool of transfer
daemons are standing by to transfer the files posted to a shared queue. The number of subscribers
is programmable and scalable, and the techniques and topology to do the transfer are all easily
controlled to optimize transfers for whatever criteria are deemed most important.</p>
</section>
<section id="problem-statement">
<h3><a class="toc-backref" href="#id5">Problem Statement</a><a class="headerlink" href="#problem-statement" title="Permalink to this headline">¶</a></h3>
<p>In November 2016, Environment and Climate Change Canada’s (ECCC) Meteorological Service of Canada (MSC),
as part of the High Performance Computing Replacement (HPCR) project asked for very large directory
trees to be mirrored in real-time. Shared Services Canada (SSC) had primary responsibility for deployment
of HPCR, with ECCC/MSC being the sole user community. It was known from the outset that these trees would be too large to
deal with using ordinary tools. It was expected that it would take about 15 months to explore the
issue and arrive at an effective operational deployment. It should be noted that SSC worked throughout
this period in close partnership with ECCC, and that this deployment required the very active participation of
sophisticated users to follow along with the twists and turns and different avenues explored and implemented.</p>
<p>The computing environment is Canada’s national weather centre, whose primary application is <em>production</em> numerical
weather prediction, where models run 7/24 hours/day running different simulations (models of the atmosphere,
and sometimes waterways and ocean, and the land under the atmosphere) either ingesting current observations
aka <em>assimilation</em>, mapping them to a grid <em>analysis</em>, and then walking the grid forward in
time <em>prediction/prognostic</em>. The forecasts follow a precise schedule throughout the 24hour cycle, feeding
into one another, so delays ripple, and considerable effort is expended to avoid interruptions and
maintain schedule.</p>
</section>
<section id="hpcr-solution-overview">
<h3><a class="toc-backref" href="#id6">HPCR Solution Overview</a><a class="headerlink" href="#hpcr-solution-overview" title="Permalink to this headline">¶</a></h3>
<a class="reference internal image-reference" href="../_images/HPC-XC_High_Availability.png"><img alt="../_images/HPC-XC_High_Availability.png" src="../_images/HPC-XC_High_Availability.png" style="width: 1078.44px; height: 846.7800000000001px;" /></a>
<p>In the diagram above, if operations are in Data Hall 1 (left of centre) and they fail, then the goal is to resume
operations promptly from Data Hall 2 (on the right). For this to be realistic, production data must be available
in the other hall, on the other site store, quickly. The <em>mirroring</em> problem is synchronizing a very large
subset of data between site store 1 and site store 2. For monitoring purposes, at the same time, a smaller
subset to must be mirrored to data hall 0.</p>
</section>
<section id="continuous-mirroring">
<h3><a class="toc-backref" href="#id7">Continuous Mirroring</a><a class="headerlink" href="#continuous-mirroring" title="Permalink to this headline">¶</a></h3>
<p>There is a pair of clusters running these simulations, one normally mostly working on operations,
and the other as a <em>spare</em> (running only research and development loads).  When the primary fails,
the intent is to run operations on the other supercomputer, using a <em>spare</em> disk to which all the
live data has been mirrored. As there are (nearly) always runs in progress, the directories never
stop being modified, and there is no maintenance period when one can catch up if one falls behind.</p>
<p>There are essentially three parts of the problem:</p>
<blockquote>
<div><ul class="simple">
<li><p>Detection: obtain the list of files which have been modified (recently).</p></li>
<li><p>Transfer: copy them to the other cluster (minimizing overhead.)</p></li>
<li><p>Performance: aspirational deadline to deliver a mirrored file: five minutes.</p></li>
</ul>
</div></blockquote>
<p>The actual trees to mirror were the following in the original contract phase (retrospectively called U0):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>psilva@eccc1-ppp1:/home/sarr111/.config/sarra/poll$ grep directory *hall1*.conf
policy_hall1_admin.conf:directory /fs/site1/ops/eccc/cmod/prod/admin
policy_hall1_archive_dbase.conf:directory /fs/site1/ops/eccc/cmod/prod/archive.dbase
policy_hall1_cmop.conf:directory /fs/site1/ops/eccc/cmod/cmop/data/maestro/smco500
policy_hall1_daily_scores.conf:directory /fs/site1/ops/eccc/cmod/prod/daily_scores
policy_hall1_hubs.conf:directory /fs/site1/ops/eccc/cmod/prod/hubs
policy_hall1_products.conf:directory /fs/site1/ops/eccc/cmod/prod/products
policy_hall1_stats.conf:directory /fs/site1/ops/eccc/cmod/prod/stats
policy_hall1_version_control.conf:directory /fs/site1/ops/eccc/cmod/prod/version_control
policy_hall1_work_ops.conf:directory /fs/site1/ops/eccc/cmod/prod/work_ops
policy_hall1_work_par.conf:directory /fs/site1/ops/eccc/cmod/prod/work_par
psilva@eccc1-ppp1:/home/sarr111/.config/sarra/poll$
</pre></div>
</div>
<p>Initially, it was known that the number of files was large, but there was no knowledge of the actual
amounts involved. Nor was that data even available until much later.</p>
<p>The most efficient way to copy these trees, as was stated at the outset, would be for all of the jobs
writing files in the trees to explicitly announce the files to be copied. This would involve users
modifying their jobs to include invocation of sr_cpost (a command which queues up file transfers for
third parties to perform). However, the client set the additional constraint that modification of user jobs was
not feasible, so the method used to obtain the list of files to copy had to be implicit (done by the
system without active user involvement).</p>
</section>
<section id="reading-the-tree-takes-too-long">
<h3><a class="toc-backref" href="#id8">Reading the Tree Takes Too Long</a><a class="headerlink" href="#reading-the-tree-takes-too-long" title="Permalink to this headline">¶</a></h3>
<p>One could just scan at a higher level in order to scan a single parent directory, but the half-dozen
sub-trees trees were picked in order to have smaller ones which worked more quickly, regardless of the
method being used to obtain lists of new files. What do we mean when we say these trees are too large?
The largest of these trees is <em>hubs</em> ( /fs/site1/ops/eccc/cmod/prod/hubs ). Rsync was run on the <em>hubs</em>
directory, as just walking the tree once, without any file copying going on. The walk of the tree, using
rsync with checksumming disabled as an optimization, resulted in the log below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>psilva@eccc1-ppp1:~/test$ more tt_walk_hubs.log
nohup: ignoring input
rsync starting @ Sat Oct  7 14:56:52 GMT 2017
number of files examined is on the order of: rsync --dry-run --links -avi --size-only /fs/site1/ops/eccc/cmod/prod/hubs /fs/site2/ops/eccc/cmod/prod/hubs |&amp; wc -l
27182247
rsync end @ Sat Oct  7 20:06:31 GMT 2017
psilva@eccc1-ppp1:~/test$
</pre></div>
</div>
<p>A <strong>single pass took over five hours, to examine 27 million files or</strong> examining <strong>about 1500 files per second.</strong>
The maximum rate of running rsyncs on this tree is thus on the order of once every six hours (to allow some
time for copying) for this tree. Note that any usual method of copying a directory tree requires traversing
it, and that there is no reason to believe that any other tool such as find, dump, tar, tree, etc… would
be significantly quicker than rsync. We need a faster method of knowing which files have been modified
so that they can be copied.</p>
</section>
<section id="detection-methods-inotify-policy-shim">
<h3><a class="toc-backref" href="#id9">Detection Methods: Inotify, Policy, SHIM</a><a class="headerlink" href="#detection-methods-inotify-policy-shim" title="Permalink to this headline">¶</a></h3>
<p>There is a Linux kernel feature known as INOTIFY, which can trigger an event when a file is modified. By
setting an INOTIFY trigger on every directory in the tree, we can be notified when any file is modified
in the tree. This was the initial approach taken. It turns out (in January 2017), that INOTIFY is indeed a
Linux feature, in that the INOTIFY events only propagate across a single server. With a cluster file
system like GPFS, one needs to run an INOTIFY monitor on every kernel where files are written. So rather
than running a single daemon, we were faced with running several hundred daemons (one per physical node),
each monitoring the same set of tens of millions of files. Since the deamons were running on many nodes,
the memory use rose into the terabyte range.</p>
<p>An alternate approach: instead of running the modification detection at the Linux level, use the file
system itself, which is database driven, to indicate which files had been modified. The HPC solution’s main
storage system uses IBM’s General Parallel File System, or GPFS. Using the <em>GPFS-policy</em> method, a query is
run against the file system database at as high a rhythm as can be sustained (around five to ten minutes per
query) combined with sr_poll to announce the files modified (and thus eligible for copying). This is
completely non-portable, but was expected to be much faster than file tree traversal.</p>
<p>Over the winter 2016/2017, both of these methods were implemented. The INOTIFY-based sr_watch was the
fastest method (instantaneous), but the daemons were having stability and memory consumption problems,
and they also took too long to startup (requires an initial tree traversal, which takes the same time
as the rsync). While slower (taking longer to notice a file was modified), the GPFS policy had <em>acceptable</em>
performance and was far more reliable than the parallel sr_watch method, and by the spring, with deployment
expected for early July 2017, the GPFS policy approach was selected.</p>
<p>As the migration progressed, the file systems grew in that they had more files in the trees, and the GPFS-policy
method progressively slowed. Already in July 2017, this was not an acceptable solution. At this point,
the idea of intercepting jobs’ file i/o calls with a shim library was introduced. ECCC told SSC
at the time, that having correct feeds, and having everything ready for transition was the
priority, so the focus of efforts was in that direction until the migration was achieved in
September. In spite of being a lower priority over the summer, a C implementation of the
sending portion of the sarra library was implemented along with a prototype shim library to call it.</p>
<p>It should be noted that the GPFS-policy runs have been operationally deployed since 2017. This has
turned out to be <em>version 1</em> of the mirroring solution, and has achieved a mirroring to secondary
clusters with approximately 20 minutes of delay in getting the data to the second system. Three years
in, there is now an upgrade of the supercomputer clusters (called U1) in progress with two new additional
clusters online, The client is now using normal Sarracenia methods to mirror from the old backup cluster
to the new ones, with only a few seconds delay beyond what it takes to get to the backup cluster.</p>
<p>It should also be noted that use of GPFS policy queries have imposed a significant and continuous
load on the GPFS clusters, and are a constant worry to the GPFS administrators. They would very much
like to get rid of it. Performance has stabilized in the past year, but it does appear to slow
as the size of the file tree grows. Many optimisations were implemented to obtain adequate
performance.</p>
<section id="shim-library">
<h4><a class="toc-backref" href="#id10">Shim Library</a><a class="headerlink" href="#shim-library" title="Permalink to this headline">¶</a></h4>
<p>The method settled on for notification is a <a class="reference external" href="https://en.wikipedia.org/wiki/Shim_(computing)">shim library</a>
When a running application makes calls to API entry points that are provided by
libraries or the kernel, there is a search process (resolved at application
load time) that finds the first entry in the path that has the proper signature.
For example, in issuing a file close(2) call, the operating system will arrange
for the correct routine in the correct library to be called.</p>
<img alt="../_images/shim_explanation_normal_close.svg" src="../_images/shim_explanation_normal_close.svg" /><p>A call to the close routine, indicates that a program has finished writing the
file in question, and so usually indicates the earliest time it is useful to
advertise a file for transfer. We created a shim library, which has entry
points that impersonate the ones being called by the application, in order
to have file availability notifications posted by the application itself,
without any application modification.</p>
<img alt="../_images/shim_explanation_shim_close.svg" src="../_images/shim_explanation_shim_close.svg" /><p>Usage of the shim library is detailed in <a class="reference external" href="../Reference/sr3.1.rst#post">sr_post(1)</a></p>
</section>
</section>
<section id="copying-files">
<h3><a class="toc-backref" href="#id11">Copying Files</a><a class="headerlink" href="#copying-files" title="Permalink to this headline">¶</a></h3>
<p>It needs to be noted that while all of this work was progressing on the ‘obtain the list of
files to be copied’ part of the problem, we were also working on the ‘copy the files to the
other side’ part of the problem. Over the summer, results of performance tests and other
considerations militated frequent changes in tactics.  The <em>site stores</em> are clusters in
their own right.  They have protocol nodes for serving traffic outside of the GPFS cluster. There are
siteio nodes with infiniband connections and actual disks.  The protocol nodes (called nfs or proto)
are participants in the GPFS cluster dedicated to i/o operations, used to offload i/o from the
main compute clusters (PPP and Supercomputer), which have comparable connections to the site store
as the protocol nodes.</p>
<p>There are multiple networks (40GigE, Infiniband, as well as management networks) and the one
to use needs to be chosen as well.  Then there are the methods of communication (ssh over tcp/ip?
bbcp over tcp/ip? GPFS over tcpip? ipoib? native-ib?).</p>
<img alt="../_images/site-store.jpg" src="../_images/site-store.jpg" />
<p>Many different sources and destinations (ppp, nfs, and protocol nodes), as well many different
methods (rcp, scp, bbcp, sscp, cp, dd) and were all trialled to different degrees at different
times. At this point several strengths of sarracenia were evident:</p>
<ul class="simple">
<li><p>The separation of publishing from subscribing means that one can subscribe on the source node
and push to the destination, or on the destination and pull from the source. It is easy to
adapt for either approach (ended up on destination protocol nodes, pulling from the source).</p></li>
<li><p>The separation of copying from the computational jobs means that the models run times are
unaffected, as the i/o jobs are completely separate.</p></li>
<li><p>The ability to scale the number of workers to the performance needed (eventually settled
on 40 workers performing copies in parallel).</p></li>
<li><p>The availability of plugins <em>download_cp</em>, <em>download_rcp</em>, <em>download_dd</em>, allow many different
copy programs (and hence protocols) to be easily applied to the transfer problem.</p></li>
</ul>
<p>Many different criteria were considered (such as: load on nfs/protocol nodes, other nodes,
transfer speed, load on PPP nodes). The final configuration selected of using <em>cp</em> (via the
<em>download_cp</em> plugin) initiated from the receiving site store’s protocol nodes.  So the reads
would occur via GPFS over IPoIB, and the writes would be done over native GPFS over IB.
This was not the fastest transfer method tested (<em>bbcp</em> was faster) but it was selected because
it spread the load out to the siteio nodes, resulted in more stable NFS and protocol
nodes and removed tcp/ip setup/teardown overhead. The ‘copy the files to the other side’ part
of the problem was stable by the end of the summer of 2017, and the impact on system stability
is minimized.</p>
</section>
<section id="shim-library-necessary">
<h3><a class="toc-backref" href="#id12">Shim Library Necessary</a><a class="headerlink" href="#shim-library-necessary" title="Permalink to this headline">¶</a></h3>
<p>Unfortunately, the mirroring between sites was running with about a 10-minute lag on the source files
system (about 30 times faster than a naive rsync approach), and was only working in principle, with
many files missing in practice, it wasn’t usable for its intended purpose. The operational commissioning of the
HPCR solution as a whole (with mirroring deferred) occurred in September of 2017, and work on mirroring essentially
stopped until October (because of activities related to the commissioning work).</p>
<p>We continued to work on two approaches, the libsrshim, and the GPFS-policy. The queries run by the GPFS-policy had to to be tuned, eventually
an overlap of 75 seconds (where a succeeding query would ask for file modifications up to a point 75 seconds before the last one
ended) because there were issues with files being missing in the copies. Even with this level of overlap, there were still missing
files. At this point, in late November, early December, the libsrshim was working well enough to be so encouraging that folks lost
interest in the GPFS policy. In contrast to an average of about a 10-minute delay starting a file copy with GPFS-policy queries,
the libsrshim approach has the copy queued as soon as the file is closed on the source file system.</p>
<p>It should be noted that when the work began, the python implementation of Sarracenia was a data distribution tool, with no support for mirroring.
As the year progressed features (symbolic link support, file attribute transportation, file removal support) were added to the initial package.
The idea of periodic processing (called heartbeats) was added, first to detect failures of clients (by seeing idle logs) but later to initiate
garbage collection for the duplicates cache, memory use policing, and complex error recovery. The use case precipitated many improvements in
the application, including a second implementation in C for environments where a Python3 environment was difficult to establish, or
where efficiency was paramount (the libsrshim case).</p>
</section>
<section id="does-it-work">
<h3><a class="toc-backref" href="#id13">Does it Work?</a><a class="headerlink" href="#does-it-work" title="Permalink to this headline">¶</a></h3>
<p>In December 2017, the software for the libsrshim approach looked ready, it was deployed in some small parallel (non-operational runs). Testing
in parallel runs started in January 2018. There were many edge cases, and testing continued for two years, until finally being
ready for deployment in December 2019. I</p>
<ul class="simple">
<li><p><strong>FIXME:</strong> include links to plugins</p></li>
<li><p><strong>FIXME:</strong> Another approach being considered is to compare file system snapshots.</p></li>
</ul>
<p>As the shim library was used in wider and wider contexts to get it closer to deployment, a significant number of edge cases
were encountered:</p>
<ul class="simple">
<li><p>use with non-login shells (especially scp) ( <a class="reference external" href="https://github.com/MetPX/sarrac/issues/66">https://github.com/MetPX/sarrac/issues/66</a>  )</p></li>
<li><p>Buggy Fortran applications improperly calling close  ( <a class="reference external" href="https://github.com/MetPX/sarrac/issues/12">https://github.com/MetPX/sarrac/issues/12</a>  )</p></li>
<li><p>tcl/tk treating any output to stderr as an failure ( <a class="reference external" href="https://github.com/MetPX/sarracenia/issues/69">https://github.com/MetPX/sarracenia/issues/69</a> )</p></li>
<li><p><em>high performance shell scripts</em> (  <a class="reference external" href="https://github.com/MetPX/sarrac/issues/15">https://github.com/MetPX/sarrac/issues/15</a> )</p></li>
<li><p>code that doesn’t close every files ( <a class="reference external" href="https://github.com/MetPX/sarrac/issues/11">https://github.com/MetPX/sarrac/issues/11</a> )</p></li>
<li><p>code that does not close even one file ( <a class="reference external" href="https://github.com/MetPX/sarrac/issues/68">https://github.com/MetPX/sarrac/issues/68</a> )</p></li>
<li><p>there are paths in use longer than 255 characters ( <a class="reference external" href="https://github.com/MetPX/sarrac/issues/39">https://github.com/MetPX/sarrac/issues/39</a> )</p></li>
<li><p>clashes in symbols, causing sed to crash ( <a class="reference external" href="https://github.com/MetPX/sarrac/issues/80">https://github.com/MetPX/sarrac/issues/80</a> )</p></li>
</ul>
<p>Over the ensuing two years, these edge cases have been dealt with and deployment finally happenned
with the transition to U1 in January 2020. It is expected that the delay in
files appearing on the second file system will be on the order of five minutes
after they are written on the source tree, or 72 times faster than rsync (see
next section for performance info), but we don´t have concrete metrics yet.</p>
<p>The question naturally arose, if the directory tree cannot be traversed, how do we know that the source and destination trees are the same?
A program to pick random files on the source tree is used to feed an sr_poll, which then adjusts the path to compare it to the same file
on the destination. Over a large number of samples, we get a quantification of how accurate the copy is. The plugin for this comparison
is still in development.</p>
</section>
<section id="is-it-fast">
<h3><a class="toc-backref" href="#id14">Is it Fast?</a><a class="headerlink" href="#is-it-fast" title="Permalink to this headline">¶</a></h3>
<p>The GPFS-policy runs are the still the method in use operationally as this is written (2018/01). The performance numbers given in
the summary are taken from the logs of one day of GPFS-policy runs.</p>
<blockquote>
<div><ul class="simple">
<li><p>Hall1 to Hall2: bytes/days: 18615163646615 = 16T, nb file/day:  1901463</p></li>
<li><p>Hall2 to CMC: bytes/days: 4421909953006 = 4T, nb file/day: 475085</p></li>
</ul>
</div></blockquote>
<p>All indications are that the shim library copies more data more quickly than the policy based runs,
but so far (2018/01) only subsets of the main tree have been tested.  On one tree of 142000 files, the GPFS-policy run had a mean
transfer time of 1355 seconds (about 23 minutes), where the shim library approach had a mean transfer time of 239 seconds (less than
five minutes) or a speedup for libshim vs. GPFS-policy of about 4:1. On a second tree where the shim library transferred 144
thousand files in a day, the mean transfer time was 264 seconds, where the same tree with the GPFS-policy approach took 1175
(basically 20 minutes). The stats are accumulated for particular hours, and at low traffic times, the average transfer time with
the shim library was 0.5 seconds vs. 166 seconds with the policy. One could claim a 300:1 speedup, but this is just inherent to
the fact that GPFS-policy method must be limited to a certain polling interval (five minutes) to limit impact on the file system,
and that provides a lower bound on transfer latency.</p>
<p>On comparable trees, the number of files being copied with the shim library is always higher than with the GPFS-policy. While
correctness is still being evaluated, the shim method is apparently working better than the policy runs. If we return to the
original rsync performance of 6 hours for the tree, then the ratio we expect to deliver on is 6 hours vs. 5 minutes …
or 72:1 speedup.</p>
<p>The above is based on the following client report:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Jan 4th
Preload:
dracette@eccc1-ppp1:~$ ./mirror.audit_filtered -c ~opruns/.config/sarra/subscribe/ldpreload.conf  -t daily -d <span class="m">2018</span>-01-04
Mean transfer time: <span class="m">238</span>.622s
Max transfer time: <span class="m">1176</span>.83s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/wcps_20170501/wh/banco/cutoff/2018010406_078_prog_gls_rel.tb0
Min transfer time: <span class="m">0</span>.0244577s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/capa25km_20170619/gridpt/qperad/radar/radprm/backup/ATX_radprm
Total files: <span class="m">142426</span>
Files over 300s: <span class="m">44506</span>
Files over 600s: <span class="m">14666</span>
Policy:
dracette@eccc1-ppp1:~$ ./mirror.audit_filtered -c ~opruns/.config/sarra/subscribe/mirror-ss1-from-hall2.conf  -t daily -d <span class="m">2018</span>-01-04
Mean transfer time: <span class="m">1355</span>.42s
Max transfer time: <span class="m">2943</span>.53s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/prod/hubs/suites/par/capa25km_20170619/gridpt/qperad/surface/201801041500_tt.obs
Min transfer time: <span class="m">1</span>.93106s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/prod/archive.dbase/dayfiles/par/2018010416_opruns_capa25km_rdpa_final
Total files: <span class="m">98296</span>
Files over 300s: <span class="m">97504</span>
Files over 600s: <span class="m">96136</span>

Jan 3rd
Preload:
dracette@eccc1-ppp1:~$ ./mirror.audit_filtered -c ~opruns/.config/sarra/subscribe/ldpreload.conf  -t daily -d <span class="m">2018</span>-01-03
Mean transfer time: <span class="m">264</span>.377s
Max transfer time: <span class="m">1498</span>.73s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/capa25km_20170619/gridpt/capa/bassin/6h/prelim/05/2018010312_05ME005_1.dbf
Min transfer time: <span class="m">0</span>.0178287s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/capa25km_20170619/gridpt/qperad/radar/statqpe/backup/XSS_0p1_statqpe
Total files: <span class="m">144419</span>
Files over 300s: <span class="m">60977</span>
Files over 600s: <span class="m">14185</span>
Policy:
dracette@eccc1-ppp1:~$ ./mirror.audit_filtered -c ~opruns/.config/sarra/subscribe/mirror-ss1-from-hall2.conf  -t daily -d <span class="m">2018</span>-01-03
Mean transfer time: <span class="m">1175</span>.33s
Max transfer time: <span class="m">2954</span>.57s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/prod/hubs/suites/par/capa25km_20170619/gridpt/qperad/surface/201801032200_tt.obs
Min transfer time: -0.359947s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/prod/hubs/suites/par/capa25km_20170619/gridpt/qperad/radar/pa/1h/XTI/201801031300~~PA,PA_PRECIPET,EE,1H:URP:XTI:RADAR:META:COR1
Total files: <span class="m">106892</span>
Files over 300s: <span class="m">106176</span>
Files over 600s: <span class="m">104755</span>

To keep <span class="k">in</span> mind:

We have <span class="m">12</span> instances <span class="k">for</span> the preload <span class="k">while</span> we’re running <span class="m">40</span> <span class="k">for</span> the policy.

* I filtered out the <span class="nb">set</span> of files that skewed the results heavily.
* The preload audit <span class="k">in</span> hourly slices shows that it’s heavily instance-bound.
* If we were to boost it up it should give out much better results <span class="k">in</span> high count situations.

Here’s Jan 4th  again but by hourly slice:


dracette@eccc1-ppp1:~$ ./mirror.audit_filtered -c ~opruns/.config/sarra/subscribe/ldpreload.conf  -t hourly -d <span class="m">2018</span>-01-04
<span class="m">00</span> GMT
Mean transfer time: <span class="m">0</span>.505439s
Max transfer time: <span class="m">5</span>.54261s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/capa25km_20170619/gridpt/qperad/radar/pa/6h/XME/201801040000~~PA,PA_PRECIPET,EE,6H:URP:XME:RADAR:META:NRML
Min transfer time: <span class="m">0</span>.0328007s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/capa25km_20170619/gridpt/qperad/radar/statqpe/backup/IWX_0p5_statqpe
Total files: <span class="m">847</span>
Files over 300s: <span class="m">0</span>
Files over 600s: <span class="m">0</span>
<span class="m">01</span> GMT
Mean transfer time: <span class="m">166</span>.883s
Max transfer time: <span class="m">1168</span>.64s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/wcps_20170501/wh/banco/cutoff/2018010318_078_prog_gls_rel.tb0
Min transfer time: <span class="m">0</span>.025425s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/capa25km_20170619/gridpt/qperad/biais/6h/XPG/201801031800_XPG_statomr
Total files: <span class="m">24102</span>
Files over 300s: <span class="m">3064</span>
Files over 600s: <span class="m">1</span>
<span class="m">02</span> GMT
Mean transfer time: <span class="m">0</span>.531483s
Max transfer time: <span class="m">4</span>.73308s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/archive.dbase/dayfiles/par/2018010401_opruns_capa25km_rdpa_preli
Min transfer time: <span class="m">0</span>.0390887s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/capa25km_20170619/gridpt/qperad/radar/radprm/XMB/201801031900_XMB_radprm
Total files: <span class="m">774</span>
Files over 300s: <span class="m">0</span>
Files over 600s: <span class="m">0</span>
<span class="m">03</span> GMT
Mean transfer time: <span class="m">0</span>.669443s
Max transfer time: <span class="m">131</span>.666s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/capa25km_20170619/gridpt/qperad/radar/pa/1h/WKR/201801032000~~PA,PA_PRECIPET,EE,1H:URP:WKR:RADAR:META:COR2
Min transfer time: <span class="m">0</span>.0244577s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/capa25km_20170619/gridpt/qperad/radar/radprm/backup/ATX_radprm
Total files: <span class="m">590</span>
Files over 300s: <span class="m">0</span>
Files over 600s: <span class="m">0</span>
<span class="m">04</span> GMT
Mean transfer time: <span class="m">59</span>.0324s
Max transfer time: <span class="m">236</span>.029s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/wcps_20170501/wf/depot/2018010400/nemo/LISTINGS/ocean.output.00016.672
Min transfer time: <span class="m">0</span>.033812s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/resps_20171107/forecast/products_dbase/images/2018010400_resps_ens-point-ETAs_239h-boxplot-NS_Pictou-001_240.png
Total files: <span class="m">2297</span>
Files over 300s: <span class="m">0</span>
Files over 600s: <span class="m">0</span>
<span class="m">05</span> GMT
Mean transfer time: <span class="m">6</span>.60841s
Max transfer time: <span class="m">28</span>.6136s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/rewps_20171018/forecast/products_dbase/images_prog/2018010400_rewps_ens-point-Hs_Tp_072h-45012-000_072.png
Min transfer time: <span class="m">0</span>.0278831s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/capa25km_20170619/gridpt/qperad/radar/statqpe/XSM/201801032200_XSM_0p2_statqpe
Total files: <span class="m">3540</span>
Files over 300s: <span class="m">0</span>
Files over 600s: <span class="m">0</span>
<span class="m">06</span> GMT
Mean transfer time: <span class="m">1</span>.90411s
Max transfer time: <span class="m">18</span>.5288s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/capa25km_20170619/gridpt/qperad/radar/statqpe/backup/ARX_0p5_statqpe
Min transfer time: <span class="m">0</span>.0346384s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/capa25km_20170619/gridpt/qperad/biais/6h/WWW/201801040600_WWW_statomr
Total files: <span class="m">757</span>
Files over 300s: <span class="m">0</span>
Files over 600s: <span class="m">0</span>
<span class="m">07</span> GMT
Mean transfer time: <span class="m">262</span>.338s
Max transfer time: <span class="m">558</span>.845s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/capa25km_20170619/gridpt/capa/bassin/6h/final/11/2018010400_11AA028_1.shp
Min transfer time: <span class="m">0</span>.028173s <span class="k">for</span> file: /space/hall2/sitestore/eccc/cmod/cmoi/opruns/ldpreload_test/hubs/suites/par/capa25km_20170619/gridpt/qperad/biais/6h/DLH/201801040000_DLH_statomr
Total files: <span class="m">23849</span>
Files over 300s: <span class="m">11596</span>
Files over 600s: <span class="m">0</span>
</pre></div>
</div>
</section>
<section id="overheads">
<h3><a class="toc-backref" href="#id15">Overheads</a><a class="headerlink" href="#overheads" title="Permalink to this headline">¶</a></h3>
<p>What is the effect on user jobs of putting the shim library in service?
When used in large models with good i/o patterns necessary for high
performance, the overhead added by the shim library can be negligeable.
However there is additional overhead introduced whenever a process is spawned,
closes a file, and when it terminates.  Shell scripts, which
function by spawning and reaping processes continuously, see maximum
impact from the shim library.  This is explored in Issue <a class="reference external" href="https://github.com/MetPX/sarrac/issues/15">https://github.com/MetPX/sarrac/issues/15</a> :</p>
<p>Issue 15 describes the worst case shell script that re-writes a file, one line
at a time, spawning and reaping a process every time. In that case, we see as
much as an 18 fold penalty in shell script performance. However re-writing
the shell script in python can yield a 20 fold improvement in performance,
with almost no overhead from the shim library (360 times faster than the
equivalent shell script with the shim library active.)</p>
<p>So shell scripts that were slow before, may be much slower with the shim
library, but the accelleration available by re-formulating to more efficient
methods can have much larger benefits as well.</p>
</section>
<section id="contributions">
<h3><a class="toc-backref" href="#id16">Contributions</a><a class="headerlink" href="#contributions" title="Permalink to this headline">¶</a></h3>
<p><strong>Dominic Racette</strong> - ECCC CMC Operations Implementation</p>
<blockquote>
<div><p>Client lead on the mirroring project. A lot of auditing and running of tests.
Integration/deployment of copying plugins. A great deal of testing and extraction of log reports.
This was a project relied extensive client participation to provide a hugely varied test suite,
and Dominic was responsible for the lion´s share of that work.</p>
</div></blockquote>
<p><strong>Anthony Chartier</strong> - ECCC CMC Development</p>
<blockquote>
<div><p>Client lead on the <em>Acquisition de Données Environnementales</em> the data acquisition system used by
Canadian numerical weather prediction suites.</p>
</div></blockquote>
<p><strong>Doug Bender</strong> - ECCC CMC Operations Implementation</p>
<blockquote>
<div><p>Another client analyst participating in the project.  Awareness, engagement, etc…</p>
</div></blockquote>
<p><strong>Daluma Sen</strong> - SSC DCSB Supercomputing HPC Optimization</p>
<blockquote>
<div><p>Building C libraries in HPC environment, contributing the random file picker, general consulting.</p>
</div></blockquote>
<p><strong>Alain St-Denis</strong> - Manager, SSC DCSB Supercomputing HPC Optimization</p>
<blockquote>
<div><p>Inspiration, consultation, wise man. Initially proposed shim library. Helped with debugging.</p>
</div></blockquote>
<p><strong>Daniel Pelissier</strong> - SSC DCSB Supercomputing HPC Integration / then replacing Alain.</p>
<blockquote>
<div><p>Inspiration/consultation on GPFS-policy work, and use of storage systems.</p>
</div></blockquote>
<p><strong>Tarak Patel</strong> - SSC DCSB Supercomputing HPC Integration.</p>
<blockquote>
<div><p>Installation of Sarracenia on protocol nodes and other specific locations. Development of GPFS-policy scripts,
called by Jun Hu’s plugins.</p>
</div></blockquote>
<p><strong>Jun Hu</strong>  - SSC DCSB Supercomputing Data Interchange</p>
<blockquote>
<div><p>Deployment lead for SSC, developed GPFS-policy Sarracenia integration plugins,
implemented them within sr_poll, worked with CMOI on deployments.
Shouldered most of SSC’s deployment load. Deployment of inotify/sr_watch implementation.</p>
</div></blockquote>
<p><strong>Noureddine Habili</strong>  - SSC DCSB Supercomputing Data Interchange</p>
<blockquote>
<div><p>Debian packaging for C-implementation. Some deployment work as well.</p>
</div></blockquote>
<p><strong>Peter Silva</strong> - Manager, SSC DCSB Supercomputing Data Interchange</p>
<blockquote>
<div><p>Project lead, wrote C implementation including shim library, hacked on the Python
also from time to time. Initial versions of most plugins (in Sarra.)</p>
</div></blockquote>
<p><strong>Michel Grenier</strong> - SSC DCSB Supercomputing Data Interchange</p>
<blockquote>
<div><p>Python Sarracenia development lead. Some C fixes as well.</p>
</div></blockquote>
<p><strong>Deric Sullivan</strong> - Manager, SSC DCSB Supercomputing HPC Solutions</p>
<blockquote>
<div><p>Consultation/work on deployments with inotify solution.</p>
</div></blockquote>
<p><strong>Walter Richards</strong> - SSC DCSB Supercomputing HPC Solutions</p>
<blockquote>
<div><p>Consultation/work on deployments with inotify solution.</p>
</div></blockquote>
<p><strong>Jamal Ayach</strong> - SSC DCSB Supercomputing HPC Solutions</p>
<blockquote>
<div><p>Consultation/work on deployments with inotify solution, also
native package installation on pre and post processors.</p>
</div></blockquote>
<p><strong>Michael Saraga</strong> - SSC DCSB .Data Interchange</p>
<blockquote>
<div><p>work on the C implementation in 2019, prepared native packaging and packages
for Suse and Redhat distributions.</p>
</div></blockquote>
<p><strong>Binh Ngo</strong> - SSC DCSB Supercomputing HPC Solutions</p>
<blockquote>
<div><p>native package installation on cray backends.</p>
</div></blockquote>
<p><strong>FIXME:</strong> who else should be here: ?</p>
<p>There was also support and oversight from management in both ECCC and SSC throughout the project.</p>
</section>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Case Study: HPC Mirroring</a><ul>
<li><a class="reference internal" href="#continuously-mirror-27-million-file-tree-very-quickly">Continuously Mirror 27 Million File Tree Very Quickly</a><ul>
<li><a class="reference internal" href="#summary">Summary</a></li>
<li><a class="reference internal" href="#problem-statement">Problem Statement</a></li>
<li><a class="reference internal" href="#hpcr-solution-overview">HPCR Solution Overview</a></li>
<li><a class="reference internal" href="#continuous-mirroring">Continuous Mirroring</a></li>
<li><a class="reference internal" href="#reading-the-tree-takes-too-long">Reading the Tree Takes Too Long</a></li>
<li><a class="reference internal" href="#detection-methods-inotify-policy-shim">Detection Methods: Inotify, Policy, SHIM</a><ul>
<li><a class="reference internal" href="#shim-library">Shim Library</a></li>
</ul>
</li>
<li><a class="reference internal" href="#copying-files">Copying Files</a></li>
<li><a class="reference internal" href="#shim-library-necessary">Shim Library Necessary</a></li>
<li><a class="reference internal" href="#does-it-work">Does it Work?</a></li>
<li><a class="reference internal" href="#is-it-fast">Is it Fast?</a></li>
<li><a class="reference internal" href="#overheads">Overheads</a></li>
<li><a class="reference internal" href="#contributions">Contributions</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/Explanation/HPC_Mirroring_Use_Case.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="nav-item nav-item-0"><a href="../index.html">sarracenia 3.00.013 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Case Study: HPC Mirroring</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2022, Peter Silva.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 4.4.0.
    </div>
  </body>
</html>